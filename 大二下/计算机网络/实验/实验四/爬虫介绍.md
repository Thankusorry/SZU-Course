该爬虫程序实现了以下核心功能：

1. **网页内容抓取**

- 自动下载指定URL的主页内容（index.html）
- 支持HTTPS协议访问
- 自动覆盖已存在的旧文件（通过StandardCopyOption.REPLACE_EXISTING）

2. **资源解析与发现**

- 解析HTML文档中的多种静态资源：
  - 图片资源（img标签）
  - JavaScript文件（script标签）
  - CSS样式表（link标签）
  - 超链接文件（a标签）
- 使用绝对URL处理机制，自动将相对路径转换为完整URL
- 通过Jsoup解析器保持与浏览器一致的资源解析逻辑

3. **智能过滤系统**

- 文件类型白名单过滤机制，支持以下扩展名：
  - 网页文件：html/htm
  - 样式表：css
  - 脚本：js
  - 图片：png/jpg/jpeg/gif/ico
  - 文档：pdf/doc/docx/xls/xlsx
- 使用正则表达式进行高效匹配（不区分大小写）

4. **本地存储管理**

- 自动创建多级目录结构（通过Files.createDirectories）
- 保持远程服务器目录结构：
  - 去除URL开头的斜杠
  - 保留路径层级关系
  - 示例：https://example.com/css/main.css 保存为 [基目录]/css/main.css

5. **下载监控与统计**

- 实时下载状态反馈：
  - 成功下载时显示文件大小
  - 失败时显示具体错误信息
- 下载结果汇总统计：
  - 发现资源总数
  - 成功下载数

6. **异常处理机制**

- 完善的错误捕获：
  - 网络连接异常
  - 文件写入错误
  - 路径创建失败
- 下载失败时程序持续运行（单个资源失败不影响整体流程）

7. **性能优化**

- 使用HashSet自动去重，避免重复下载
- 流式传输（InputStream）减少内存消耗
- 多类型资源并行处理（通过统一处理机制）