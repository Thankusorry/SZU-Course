# AI爬虫怎么榨干互联网

## 案例引入

### 案例1

世界上最大的免费百科--维基百科把数据整理好，作为JSON文件发布，供AI训练

原因：AI爬虫使得维基百科的带宽成本增加50%

### 案例2

Mozila根据后台数据发现网站流量前五名全是爬虫，占了网站七成流量
爬虫在DDos整个互联网

#### **疑问：AI爬虫是怎么吞噬互联网**

## 爬虫背景

**其实网络已遍布爬虫**

### 搜索引擎爬虫

搜索引擎爬虫 爬取 网站数据 从而用户能够通过搜索找到对应的信息

**爬虫得到数据的同时，也给网络带来用户**

**网站需要爬虫，但又害怕被恶意访问**

网站通过robots.txt的“君子协议”来维护和爬虫的平衡关系

这是一种成本最低的共识形式，规定了哪些内容你可以爬

全球超过五亿个网站在使用robots.txt

### **AI爬虫的出现**

AI爬虫打破了这种搜索引擎时代爬虫和网站之间脆弱的“生态关系”

1. AI爬虫更加贪婪，无差别爬取所有内容：大模型训练理论上数据越多越好，造成网站相当多的资源浪费
2. 另一方卖弄，被AI爬虫的数据给网站带来的曝光远远没有搜索引擎那么直接：比较良心的还会在联网搜索中放上链接，大多数只是把网站的数据学习完后变成自己的话复述出来，跟网站没任何关系

### **案例补充**

最近比较热门的发布大模型Claude的公司Anthropic，它过去就曾使用爬虫爬取一个叫IFixit的网站，尽管它已经明确说明*严禁因为任何其他目的复制内容*，但还是逃不过爬虫每分钟都有数千次请求访问。

## 为什么爬虫不遵守robots.txt协议？


## AI爬虫的能力不断提升（举一些主流工具），网站反爬的能力也在提升（CloudFlare），怎么看待这种关系？****


互联网-爬虫-网站

AI需要爬虫，互联网需要AI，爬虫依赖于互联网

怎么平衡爬虫和网站之间的关系需要得到解决。


## 结尾

